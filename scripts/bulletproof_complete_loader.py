#!/usr/bin/env python3
"""
BULLETPROOF COMPLETE LOADER
Goal: Load ALL 5M records with zero data loss, fixing column alignment issues
"""

import os
import sys
import time
import psycopg2
import pandas as pd
import csv
import tempfile
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path

# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from config import get_db_config

# Set CSV limits
csv.field_size_limit(2147483647)

def process_chunk_bulletproof(chunk_data, field_mapping, chunk_num):
    """Bulletproof chunk processing with enhanced validation"""
    try:
        print(f"üîß Worker processing chunk {chunk_num}: {len(chunk_data):,} rows")
        
        # CRITICAL: Verify we have the expected columns
        expected_columns = 449
        if len(chunk_data.columns) != expected_columns:
            print(f"‚ö†Ô∏è  Chunk {chunk_num}: Column count mismatch! Got {len(chunk_data.columns)}, expected {expected_columns}")
            return None, chunk_num
        
        # Create clean mapping with bulletproof column validation
        clean_data = pd.DataFrame()
        
        # Process each field individually to prevent misalignment
        for tsv_col, db_col in field_mapping.items():
            if tsv_col in chunk_data.columns:
                # Extract the specific column
                col_data = chunk_data[tsv_col].copy()
                
                # Data type specific cleaning
                if db_col in ['latitude', 'longitude']:
                    # Coordinates: Must be numeric
                    col_data = pd.to_numeric(col_data.replace('', pd.NA), errors='coerce')
                    
                elif db_col in ['building_area_total', 'lot_size_square_feet']:
                    # Numeric fields
                    col_data = pd.to_numeric(col_data.replace('', pd.NA), errors='coerce')
                    
                elif db_col in ['year_built', 'number_of_bedrooms', 'estimated_value', 'lsale_price']:
                    # Integer fields - handle floats like "1975.0" and large decimals like "30.632601"
                    # First convert to numeric, replacing empty strings with NaN
                    col_data = pd.to_numeric(col_data.replace(['', ' ', 'nan', 'NaN'], pd.NA), errors='coerce')
                    
                    # Convert float to int, handling NaN and large decimals properly
                    def safe_int_convert(x):
                        if pd.isna(x) or x is None or str(x).lower() == 'nan':
                            return None  # Explicitly return None for NaN values
                        try:
                            # For very large decimals, round to nearest integer
                            float_val = float(x)
                            if pd.isna(float_val):  # Double-check for NaN after float conversion
                                return None
                            return int(round(float_val))
                        except (ValueError, OverflowError, TypeError):
                            return None
                    
                    col_data = col_data.apply(safe_int_convert)
                    # Ensure it's treated as object type to avoid pandas automatic float conversion
                    col_data = pd.Series(col_data, dtype='object')
                    
                elif db_col == 'property_state':
                    # State: Must be 2 characters or NULL
                    col_data = col_data.fillna('').astype(str)
                    col_data = col_data.apply(lambda x: x[:2] if x and len(x) >= 2 else None)
                    
                else:
                    # String fields
                    col_data = col_data.fillna('').astype(str)
                    # Remove null characters
                    col_data = col_data.str.replace('\x00', '', regex=False)
                    col_data = col_data.apply(lambda x: None if x == '' else x)
                
                # Add to clean DataFrame with explicit column assignment
                clean_data[db_col] = col_data
            else:
                print(f"‚ö†Ô∏è  Missing column {tsv_col} in chunk {chunk_num}")
        
        # Final validation before return
        if len(clean_data) == 0:
            print(f"‚ùå Chunk {chunk_num}: No data after cleaning")
            return None, chunk_num
        
        # Validate critical coordinates
        if 'latitude' in clean_data.columns and 'longitude' in clean_data.columns:
            lat_issues = clean_data['latitude'].apply(lambda x: isinstance(x, str) and not str(x).replace('.','').replace('-','').isdigit())
            lon_issues = clean_data['longitude'].apply(lambda x: isinstance(x, str) and not str(x).replace('.','').replace('-','').isdigit())
            
            if lat_issues.any() or lon_issues.any():
                print(f"‚ö†Ô∏è  Chunk {chunk_num}: Coordinate validation issues detected")
                # Log the problematic rows
                problem_rows = lat_issues | lon_issues
                if problem_rows.any():
                    print(f"    Problem rows: {problem_rows.sum()}")
        
        return clean_data, chunk_num
        
    except Exception as e:
        print(f"‚ùå Chunk {chunk_num} error: {e}")
        import traceback
        traceback.print_exc()
        return None, chunk_num

def bulletproof_bulk_insert(clean_data, chunk_num):
    """Enhanced bulk insert with better error handling"""
    try:
        if clean_data is None or len(clean_data) == 0:
            print(f"‚ö†Ô∏è  Chunk {chunk_num}: No data to insert")
            return 0
        
        # Create temporary file with bulletproof formatting
        tmp_file_path = None
        try:
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', 
                                           newline='', encoding='utf-8') as tmp_file:
                # Replace NaN and None with proper NULL representation
                clean_data_copy = clean_data.copy()
                
                # Ensure integer fields are properly formatted (no decimal places)
                integer_fields = ['year_built', 'number_of_bedrooms', 'estimated_value', 'lsale_price']
                for field in integer_fields:
                    if field in clean_data_copy.columns:
                        # Convert to string representation without decimals for integers
                        def format_integer_field(x):
                            if pd.isna(x) or x is None or str(x).lower() in ['nan', 'none', '']:
                                return '\\N'  # PostgreSQL NULL
                            try:
                                # Handle large numbers properly
                                int_val = int(float(x))  # Convert via float first to handle decimals
                                return str(int_val)
                            except (ValueError, OverflowError, TypeError):
                                return '\\N'  # Return NULL on any conversion error
                        
                        clean_data_copy[field] = clean_data_copy[field].apply(format_integer_field)
                        
                        # Debug: Show sample of what we're about to write for this field
                        if field == 'lsale_price':
                            sample_values = clean_data_copy[field].head(5).tolist()
                            print(f"   üîç {field} formatted sample: {sample_values}")
                            
                            # COMPREHENSIVE VALIDATION: Check ALL values in the chunk
                            all_values = clean_data_copy[field].tolist()
                            problematic_values = []
                            for i, val in enumerate(all_values):
                                # Check if any value still contains decimals or invalid formats
                                if val != '\\N' and isinstance(val, str):
                                    if '.' in val and val != '\\N':
                                        problematic_values.append((i, val))
                                    elif not val.isdigit() and val != '\\N' and not (val.startswith('-') and val[1:].isdigit()):
                                        problematic_values.append((i, val))
                            
                            if problematic_values:
                                print(f"   üö® FOUND {len(problematic_values)} problematic lsale_price values!")
                                for i, (row_idx, bad_val) in enumerate(problematic_values[:10]):  # Show first 10
                                    print(f"      Row {row_idx}: '{bad_val}'")
                                    # FIX THEM ON THE SPOT
                                    try:
                                        if '.' in str(bad_val):
                                            fixed_val = str(int(round(float(bad_val))))
                                            clean_data_copy.iloc[row_idx, clean_data_copy.columns.get_loc(field)] = fixed_val
                                            print(f"      ‚úÖ Fixed: '{bad_val}' ‚Üí '{fixed_val}'")
                                    except:
                                        clean_data_copy.iloc[row_idx, clean_data_copy.columns.get_loc(field)] = '\\N'
                                        print(f"      ‚úÖ Fixed: '{bad_val}' ‚Üí '\\N'")
                                print(f"   üîß Fixed all {len(problematic_values)} problematic values")
                            else:
                                print(f"   ‚úÖ All {len(all_values)} lsale_price values are properly formatted")
                                
                                # ADDITIONAL CHECK: Verify the exact CSV content that will be written
                                if field == 'lsale_price':
                                    print(f"   üîç EXTRA VALIDATION: Checking actual CSV values...")
                                    # Check around line 7975 (the problematic line from error)
                                    target_lines = [7970, 7974, 7975, 7976, 7980]  # Around the problem area
                                    for line_idx in target_lines:
                                        if line_idx < len(clean_data_copy):
                                            csv_val = clean_data_copy.iloc[line_idx, clean_data_copy.columns.get_loc(field)]
                                            print(f"      Line {line_idx}: '{csv_val}'")
                                            # Double-check this specific value
                                            if isinstance(csv_val, str) and '.' in csv_val and csv_val != '\\N':
                                                print(f"      üö® FOUND DECIMAL IN CSV PREP: '{csv_val}' at line {line_idx}")
                                                # Fix it immediately
                                                try:
                                                    fixed = str(int(round(float(csv_val))))
                                                    clean_data_copy.iloc[line_idx, clean_data_copy.columns.get_loc(field)] = fixed
                                                    print(f"      üîß EMERGENCY FIX: '{csv_val}' ‚Üí '{fixed}'")
                                                except:
                                                    clean_data_copy.iloc[line_idx, clean_data_copy.columns.get_loc(field)] = '\\N'
                                                    print(f"      üîß EMERGENCY FIX: '{csv_val}' ‚Üí '\\N'")
                
                # Handle all other NaN/None values
                clean_data_copy = clean_data_copy.fillna('\\N')  # PostgreSQL NULL
                
                # Write CSV without float formatting for integer fields
                clean_data_copy.to_csv(tmp_file, sep='\t', header=False, index=False, 
                                     na_rep='\\N')  # Removed float_format since we handle integers manually
                tmp_file_path = tmp_file.name
        except Exception as e:
            print(f"‚ùå Error creating temp file: {e}")
            return 0
        
        # Connect and insert
        conn = psycopg2.connect(**get_db_config())
        cursor = conn.cursor()
        cursor.execute("SET search_path TO datnest, public")
        
        print(f"üíæ Inserting {len(clean_data)} records (Chunk {chunk_num})")
        print(f"   Columns: {list(clean_data.columns)}")
        
        with open(tmp_file_path, 'r', encoding='utf-8') as f:
            try:
                cursor.copy_from(f, 'properties', columns=tuple(clean_data.columns), 
                               sep='\t', null='\\N')
                conn.commit()
                print(f"‚úÖ Successfully inserted {len(clean_data)} records (Chunk {chunk_num})")
                
            except Exception as insert_error:
                print(f"‚ùå Insert error for chunk {chunk_num}: {insert_error}")
                conn.rollback()
                
                # Try to diagnose the specific row issue
                print(f"üîç Diagnosing chunk {chunk_num} data...")
                if 'latitude' in clean_data.columns:
                    lat_sample = clean_data['latitude'].head(3)
                    print(f"   Latitude sample: {lat_sample.tolist()}")
                if 'longitude' in clean_data.columns:
                    lon_sample = clean_data['longitude'].head(3)
                    print(f"   Longitude sample: {lon_sample.tolist()}")
                if 'property_state' in clean_data.columns:
                    state_sample = clean_data['property_state'].head(3)
                    print(f"   State sample: {state_sample.tolist()}")
                if 'year_built' in clean_data.columns:
                    year_sample = clean_data['year_built'].head(3)
                    print(f"   Year built sample: {year_sample.tolist()}")
                if 'lsale_price' in clean_data.columns:
                    price_sample = clean_data['lsale_price'].head(3)
                    print(f"   Sale price sample: {price_sample.tolist()}")
                    # Also check data types
                    print(f"   Sale price types: {[type(x) for x in price_sample.tolist()]}")
                
                # Show what was actually written to the CSV file for debugging
                print(f"   üìÑ Checking CSV file contents...")
                try:
                    with open(tmp_file_path, 'r', encoding='utf-8') as debug_file:
                        lines = debug_file.readlines()[:3]  # Read first 3 lines
                        for i, line in enumerate(lines):
                            fields = line.strip().split('\t')
                            print(f"   üìù CSV Line {i+1}: {len(fields)} fields")
                            # Find lsale_price column (should be around index 10-12)
                            if 'lsale_price' in clean_data.columns:
                                col_index = list(clean_data.columns).index('lsale_price')
                                if col_index < len(fields):
                                    print(f"   üí∞ lsale_price CSV value: '{fields[col_index]}'")
                except Exception as debug_error:
                    print(f"   ‚ö†Ô∏è  Debug file read error: {debug_error}")
                
                cursor.close()
                conn.close()
                
                # Safe cleanup on error
                try:
                    if tmp_file_path and os.path.exists(tmp_file_path):
                        os.unlink(tmp_file_path)
                except Exception:
                    pass  # Ignore cleanup errors in error path
                
                return 0
        
        cursor.close()
        conn.close()
        
        # Safe file cleanup
        try:
            if tmp_file_path and os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)
        except Exception as cleanup_error:
            print(f"‚ö†Ô∏è  Temp file cleanup warning: {cleanup_error}")
        
        return len(clean_data)
        
    except Exception as e:
        print(f"‚ùå Bulk insert error for chunk {chunk_num}: {e}")
        try:
            if 'tmp_file_path' in locals() and tmp_file_path and os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)
        except:
            pass
        return 0

def bulletproof_load_complete_file(tsv_file_path, max_workers=6, chunk_size=25000):
    """Load complete TSV file with bulletproof processing"""
    
    print("üõ°Ô∏è  BULLETPROOF COMPLETE LOADER")
    print("üéØ Goal: Load ALL 5,000,000 records with ZERO data loss")
    print(f"‚öôÔ∏è  Workers: {max_workers}")
    print(f"üì¶ Chunk size: {chunk_size:,}")
    print("=" * 70)
    
    # Enhanced field mapping (same as working, but with validation)
    field_mapping = {
        'Quantarium_Internal_PID': 'quantarium_internal_pid',
        'Assessors_Parcel_Number': 'apn', 
        'FIPS_Code': 'fips_code',
        'ESTIMATED_VALUE': 'estimated_value',
        'Property_City_Name': 'property_city_name',
        'Current_Owner_Name': 'current_owner_name',
        'LotSize_Square_Feet': 'lot_size_square_feet',
        'Building_Area_1': 'building_area_total',
        'Number_of_Bedrooms': 'number_of_bedrooms',
        'Year_Built': 'year_built',
        'LSale_Price': 'lsale_price',
        'PA_Latitude': 'latitude',
        'PA_Longitude': 'longitude',
        'Property_State': 'property_state',
        'Property_Full_Street_Address': 'property_full_street_address',
        'Property_Zip_Code': 'property_zip_code',
        'Owner_Occupied': 'owner_occupied',
        'Total_Assessed_Value': 'total_assessed_value'
    }
    
    start_time = time.time()
    
    try:
        # Clear database
        conn = psycopg2.connect(**get_db_config())
        cursor = conn.cursor()
        cursor.execute("TRUNCATE TABLE datnest.properties RESTART IDENTITY CASCADE")
        conn.commit()
        cursor.close()
        conn.close()
        print("‚úÖ Database cleared")
        
        # Read and process file
        total_loaded = 0
        successful_chunks = 0
        failed_chunks = 0
        
        # Use sequential processing to avoid DataFrame alignment issues
        print("üìä Processing file sequentially for maximum reliability...")
        
        chunk_reader = pd.read_csv(
            tsv_file_path,
            sep='\t',
            chunksize=chunk_size,
            dtype=str,
            encoding='utf-8',
            engine='python',
            quoting=csv.QUOTE_NONE,
            on_bad_lines='skip',
            na_values=['']
        )
        
        for chunk_num, chunk in enumerate(chunk_reader, 1):
            print(f"\nüì¶ Processing chunk {chunk_num}...")
            
            # Process chunk
            clean_data, _ = process_chunk_bulletproof(chunk, field_mapping, chunk_num)
            
            if clean_data is not None:
                # Insert chunk
                inserted = bulletproof_bulk_insert(clean_data, chunk_num)
                if inserted > 0:
                    total_loaded += inserted
                    successful_chunks += 1
                    print(f"‚úÖ Chunk {chunk_num}: {inserted:,} records loaded successfully")
                else:
                    failed_chunks += 1
                    print(f"‚ùå Chunk {chunk_num}: Failed to load")
            else:
                failed_chunks += 1
                print(f"‚ùå Chunk {chunk_num}: Failed to process")
            
            # Progress update every 10 chunks
            if chunk_num % 10 == 0:
                elapsed = time.time() - start_time
                rate = total_loaded / elapsed if elapsed > 0 else 0
                print(f"\nüìä Progress: {chunk_num} chunks, {total_loaded:,} records, {rate:,.0f} rec/sec")
        
        elapsed = time.time() - start_time
        rate = total_loaded / elapsed if elapsed > 0 else 0
        
        print(f"\nüéâ BULLETPROOF LOAD COMPLETE!")
        print(f"üìä Total Records: {total_loaded:,}")
        print(f"‚úÖ Successful Chunks: {successful_chunks}")
        print(f"‚ùå Failed Chunks: {failed_chunks}")
        print(f"‚è±Ô∏è  Time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)")
        print(f"üöÄ Rate: {rate:,.0f} records/second")
        
        # Final validation
        if total_loaded >= 4999000:  # Allow for small data quality filtering
            print(f"üéØ SUCCESS: {total_loaded:,} records loaded (target: 5,000,000)")
        else:
            print(f"‚ö†Ô∏è  INCOMPLETE: {total_loaded:,} records loaded (missing: {5000000 - total_loaded:,})")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Bulletproof load failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    tsv_path = r"C:\DataNest-TSV-Files\extracted-tsv\Quantarium_OpenLien_20250414_00001.TSV"
    
    # Use fewer workers for maximum stability
    max_workers = 1  # Sequential processing for debugging
    
    print(f"üõ°Ô∏è  BULLETPROOF MODE: Sequential processing for maximum reliability")
    
    success = bulletproof_load_complete_file(tsv_path, max_workers=max_workers, chunk_size=25000)
    exit(0 if success else 1) 