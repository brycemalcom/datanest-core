#!/usr/bin/env python3
"""
COMPLETE FINAL 54 FIELDS - TRIPLE-LOCK METHODOLOGY
Systematic completion for 100% data capture (395 â†’ 449 fields)

TRIPLE-LOCK PROCESS:
Step 1: Evidence (TSV headers confirmed)
Step 2: Foundation (Database columns verified - 505 available)
Step 3: Engine (Loader field mapping update)
Step 4: Validation (Test with real data)
"""

import os
import sys
import csv

# Add src directory to path  
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

import psycopg2
from config import get_db_config

def complete_final_54_fields():
    """Complete the final 54 fields for 100% data capture"""
    
    print("ğŸš€ COMPLETE FINAL 54 FIELDS - TRIPLE-LOCK METHODOLOGY")
    print("ğŸ¯ Goal: 100% Data Capture (395 â†’ 449 fields)")
    print("=" * 60)
    
    # =====================================================
    # STEP 1: EVIDENCE - TSV Headers Verification
    # =====================================================
    
    print("ğŸ“‚ STEP 1: EVIDENCE - TSV Headers Verification")
    
    # The 54 missing fields identified from audit
    missing_fields = {
        # Financing/Mortgage (15 fields)
        'Mtg02_Purpose_Of_Loan': 'mtg02_purpose_of_loan',
        'Mtg02_loan_number': 'mtg02_loan_number', 
        'Mtg03_due_date': 'mtg03_due_date',
        'Mtg03_lender_type': 'mtg03_lender_type',
        'Mtg03_loan_number': 'mtg03_loan_number',
        'Mtg03_type_financing': 'mtg03_type_financing',
        'Mtg04_loan_number': 'mtg04_loan_number',
        'Additional_Open_Lien_Count': 'additional_open_lien_count',
        'Total_Open_Lien_Count': 'total_open_lien_count', 
        'Total_Open_Lien_Balance': 'total_open_lien_balance',
        'Additional_Open_Lien_Balance': 'additional_open_lien_balance',
        'Current_Est_LTV_Combined': 'current_est_ltv_combined',
        'Current_Est_Equity_Dollars': 'current_est_equity_dollars',
        'Total_Financing_History_Count': 'total_financing_history_count',
        'Current_Est_LTV_Range_Code': 'current_est_ltv_range_code',
        
        # County Values/Taxes (7 fields)
        'School_Tax_District_1': 'school_tax_district_1',
        'School_Tax_District_1_Indicator': 'school_tax_district_1_indicator',
        'School_Tax_District_2': 'school_tax_district_2',
        'School_Tax_District_2_Indicator': 'school_tax_district_2_indicator', 
        'School_Tax_District_3': 'school_tax_district_3',
        'School_Tax_District_3_Indicator': 'school_tax_district_3_indicator',
        'Market_Value_Year': 'market_value_year',
        
        # Ownership (11 fields)
        'CO_Mail_Care_of_Name': 'co_mail_care_of_name',
        'CO_Mailing_Zip_Plus4Code': 'co_mailing_zip_plus4code',
        'CO_Unit_Number': 'co_unit_number', 
        'CO_Unit_Type': 'co_unit_type',
        'Mail_Care_Of_Name_Indicator': 'mail_care_of_name_indicator',
        'ParsedOwnerSourceCode': 'parsed_owner_source_code',
        'Buyer_ID_Code_1': 'buyer_id_code_1',
        'Buyer_Vesting_Code': 'buyer_vesting_code',
        'Length_of_Residence_Code': 'length_of_residence_code',
        'Ownership_Start_Date': 'ownership_start_date',
        'Owner2Firstname': 'owner2_first_name',
        
        # Land Characteristics (5 fields)
        'LotSize_Depth_Feet': 'lot_size_depth_feet',
        'LotSize_Frontage_Feet': 'lot_size_frontage_feet',
        'Lot_Size_Area_Unit': 'lot_size_area_unit',
        'Lot_Size_or_Area': 'lot_size_or_area',
        'Original_Lot_Size_or_Area': 'original_lot_size_or_area',
        
        # Property Location (3 fields)
        'Property_Street_Direction_Left': 'property_street_direction_left',
        'Property_Street_Direction_Right': 'property_street_direction_right', 
        'Property_Street_Suffix': 'property_street_suffix',
        
        # Property Legal/Other (remaining fields)
        'Duplicate_APN': 'duplicate_apn',
        'Location_Code': 'location_code',
        'Tax_Account_Number': 'tax_account_number',
        'Property_Zip_Plus4Code': 'property_zip_plus4code',
        'PA_Carrier_Route': 'pa_carrier_route',
        'PA_Census_Tract': 'pa_census_tract',
        'Match_Code': 'match_code',
        'Current_Est_Equity_Range_Code': 'current_est_equity_range_code',
        'Owner2LastName': 'owner2_last_name',
        'Site_Influence': 'site_influence',
        'Land_Use_General': 'land_use_general',
        'Lot_Size_Depth_Feet': 'lot_size_depth_feet_alt',
        'Lot_Size_Frontage_Feet': 'lot_size_frontage_feet_alt'
    }
    
    print(f"âœ… Evidence complete: {len(missing_fields)} missing fields identified")
    
    # Verify fields exist in TSV
    tsv_file = r'C:\DataNest-TSV-Files\extracted-tsv\Quantarium_OpenLien_20250414_00001.TSV'
    
    try:
        with open(tsv_file, 'r', encoding='utf-8') as f:
            reader = csv.reader(f, delimiter='\t')
            tsv_headers = next(reader)
        
        # Check which missing fields actually exist in TSV
        tsv_set = set(tsv_headers)
        verified_fields = {}
        
        for tsv_field, db_field in missing_fields.items():
            if tsv_field in tsv_set:
                verified_fields[tsv_field] = db_field
        
        print(f"ğŸ“Š TSV verification: {len(verified_fields)}/{len(missing_fields)} fields found in TSV")
        
    except Exception as e:
        print(f"âŒ TSV verification failed: {e}")
        return False
    
    # =====================================================
    # STEP 2: FOUNDATION - Database Columns Check
    # =====================================================
    
    print(f"\nğŸ—„ï¸  STEP 2: FOUNDATION - Database Columns Check")
    
    try:
        db_config = get_db_config()
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        cursor.execute("SET search_path TO datnest, public")
        
        # Check which database columns already exist
        cursor.execute("""
            SELECT column_name, data_type 
            FROM information_schema.columns 
            WHERE table_name = 'properties' 
            AND table_schema = 'datnest'
            ORDER BY column_name;
        """)
        
        existing_columns = {row[0]: row[1] for row in cursor.fetchall()}
        
        # Check mapping readiness
        mappable_fields = {}
        missing_columns = []
        
        for tsv_field, db_field in verified_fields.items():
            if db_field in existing_columns:
                mappable_fields[tsv_field] = db_field
            else:
                missing_columns.append(db_field)
        
        print(f"âœ… Foundation status: {len(mappable_fields)} fields can map to existing columns")
        print(f"âš ï¸  Missing columns: {len(missing_columns)} need to be created")
        
        if missing_columns:
            print(f"ğŸ“‹ Missing database columns:")
            for col in missing_columns[:10]:  # Show first 10
                print(f"   - {col}")
            if len(missing_columns) > 10:
                print(f"   ... and {len(missing_columns)-10} more")
        
    except Exception as e:
        print(f"âŒ Database check failed: {e}")
        return False
    
    # =====================================================
    # STEP 3: ENGINE - Loader Mapping Update
    # =====================================================
    
    print(f"\nğŸ”§ STEP 3: ENGINE - Loader Mapping Update")
    
    # Generate the field mapping addition for the loader
    print(f"ğŸ“‹ NEW FIELD MAPPINGS TO ADD ({len(mappable_fields)} fields):")
    print(f"# Add to field_mapping dictionary in enhanced_production_loader_batch4a.py:")
    print()
    
    # Group by category for organized addition
    categories = {
        'Financing/Mortgage': [],
        'County Values/Taxes': [],
        'Ownership': [],
        'Land Characteristics': [], 
        'Property Location': [],
        'Other': []
    }
    
    for tsv_field, db_field in sorted(mappable_fields.items()):
        if 'Mtg' in tsv_field or 'LTV' in tsv_field or 'Lien' in tsv_field:
            categories['Financing/Mortgage'].append((tsv_field, db_field))
        elif 'Tax' in tsv_field or 'School' in tsv_field or 'Market_Value' in tsv_field:
            categories['County Values/Taxes'].append((tsv_field, db_field))
        elif 'Owner' in tsv_field or 'CO_' in tsv_field or 'Buyer' in tsv_field:
            categories['Ownership'].append((tsv_field, db_field))
        elif 'Lot' in tsv_field or 'Site_' in tsv_field or 'Land_' in tsv_field:
            categories['Land Characteristics'].append((tsv_field, db_field))
        elif 'Property_' in tsv_field and ('Street' in tsv_field or 'Address' in tsv_field):
            categories['Property Location'].append((tsv_field, db_field))
        else:
            categories['Other'].append((tsv_field, db_field))
    
    mapping_code = []
    for category, fields in categories.items():
        if fields:
            mapping_code.append(f"        # {category} - Final completion ({len(fields)} fields)")
            for tsv_field, db_field in fields:
                mapping_code.append(f"        '{tsv_field}': '{db_field}',")
            mapping_code.append("")
    
    print("\n".join(mapping_code))
    
    # =====================================================
    # STEP 4: VALIDATION STRATEGY
    # =====================================================
    
    print(f"\nâœ… STEP 4: VALIDATION STRATEGY")
    
    print(f"ğŸ¯ COMPLETION ROADMAP:")
    print(f"   ğŸ“Š Current: 395 fields mapped (88.0%)")
    print(f"   â• Adding: {len(mappable_fields)} new field mappings")
    print(f"   ğŸ¯ Target: 449 total fields (100.0%)")
    print(f"   ğŸ“ˆ New coverage: {(395 + len(mappable_fields))/449*100:.1f}%")
    
    if missing_columns:
        print(f"   âš ï¸  Database gaps: {len(missing_columns)} columns need creation")
    
    # Generate completion strategy
    print(f"\nğŸš€ IMPLEMENTATION STEPS:")
    print(f"   1. âœ… Evidence verified ({len(verified_fields)} TSV fields confirmed)")
    print(f"   2. âœ… Foundation checked ({len(mappable_fields)} can map immediately)")
    
    if missing_columns:
        print(f"   3. ğŸ”§ Create {len(missing_columns)} missing database columns")
        print(f"   4. ğŸ”§ Update loader with {len(mappable_fields)} + {len(missing_columns)} field mappings")
    else:
        print(f"   3. ğŸ”§ Update loader with {len(mappable_fields)} field mappings")
    
    print(f"   4. ğŸ§ª Test with sample data (1000 records)")
    print(f"   5. ğŸ“Š Validate 449/449 field completion")
    print(f"   6. ğŸš€ Production deployment")
    
    # =====================================================
    # IMPLEMENTATION AUTOMATION
    # =====================================================
    
    print(f"\nğŸ¤– AUTOMATED IMPLEMENTATION READY")
    
    success_rate = len(mappable_fields) / len(verified_fields) * 100 if verified_fields else 0
    
    if success_rate >= 80:
        print(f"âœ… HIGH SUCCESS PROBABILITY: {success_rate:.0f}% fields ready for immediate mapping")
        print(f"ğŸš€ Recommended: Proceed with loader update")
    else:
        print(f"âš ï¸  MODERATE SUCCESS: {success_rate:.0f}% fields ready")
        print(f"ğŸ”§ Recommended: Address database gaps first")
    
    cursor.close()
    conn.close()
    
    return len(mappable_fields) >= 40  # Success if we can map most fields

if __name__ == "__main__":
    success = complete_final_54_fields()
    
    if success:
        print(f"\nğŸ‰ TRIPLE-LOCK ANALYSIS COMPLETE - Ready for 100% data capture!")
        print(f"ğŸš€ Next: Apply field mappings to loader for final completion")
    else:
        print(f"\nâš ï¸  Additional database preparation needed before completion") 